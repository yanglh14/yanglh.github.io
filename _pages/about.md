---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I work at Noah's Ark Lab as a Robotics Researcher in Hong Kong since Sep. 2024.

I graduated from the Department of Computer Science, University of Hong Kong (Hong Kong) with a PhD degree under supervision of Dr. Pan Jia (HKU) and Dr. Song Chaoyang (Sustech) in Aug. 2024. 
I received my B.S. degree from the Department of Precise Instrument, Tsinghua University (Beijing) in 2018. 
I worked with Dr. Huang Bidan from Tencent RoboticsX from April 2021 to July 2022 with the Tencent Rhino-Bird Elite Talent Program.

My research focuses on **robotics perception and manipulation** using learning-based methods. Specifically, I am interested in:
Developing a generic representation for world understanding through vision and tactile sensing;
Enhancing decision-making in unstructured environments with rich interactions.

# üî• News
- *2024.09*: &nbsp;üéâüéâ I join **Noah's Ark Lab** as a Robotics Researcher in Hong Kong.
- *2024.08*: &nbsp;üéâüéâ Our paper "One Fling to Goal: Environment-aware Dynamics for Goal-conditioned Fabric Flinging
" is accepted by **WAFR 2024**.

# üìñ Educations
- *2019.06 - 2024.08*, University of Hong Kong (Hong Kong), Ph.D. in Computer Science.
- *2014.09 - 2018.06*, Tsinghua University (Beijing), B.S. in Department of Precise Instrument.

# üíª <span id="work-experience">Work Experience</span>

- *Jun. 2023 - Aug. 2024*, Research Intern, Center for Transformative Garment Production, Hong Kong
- *Apr. 2021 - Jul. 2022*, Research Intern, Tencent RoboticsX, Shenzhen, China
- *Nov. 2018 ‚Äì Aug. 2019*, Research Assistant, Southern University of Science and Technology Shenzhen, China

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WAFR 2024</div><img src='images/Fling-to-goal.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[One Fling to Goal: Environment-aware Dynamics for Goal-conditioned Fabric Flinging](https://arxiv.org/pdf/2406.14136)

**Linhan Yang**, Lei Yang, Haoran Sun, Zeqing Zhang, Haibin He, Fang Wan, Chaoyang Song, Jia Pan

The 16th International Workshop on the Algorithmic Foundations of Robotics Chicago, USA, October 7-9 2024

[Paper](https://arxiv.org/pdf/2406.14136)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Biomimetics 2023</div><img src='images/biomimetics.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Bridging Locomotion and Manipulation Using Reconfigurable Robotic Limbs via Reinforcement](https://www.mdpi.com/2313-7673/8/4/364)

Haoran Sun\*, **Linhan Yang**\*, Yuping Gu, Jia Pan, Fang Wan, Chaoyang Song

Biomimetics, 2023

[Paper](https://www.mdpi.com/2313-7673/8/4/364)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">RAL 2023</div><img src='images/TacGNN.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Tacgnn: Learning tactile-based in-hand manipulation with a blind robot using hierarchical graph neural network](https://bionicdl.ancorasir.com/wp-content/uploads/2023/11/2023-J-RAL-TacGNN.pdf)

**Linhan Yang**, Bidan Huang, Qingbiao Li, Ya-Yen Tsai, Wang Wei Lee, Chaoyang Song, Jia Pan

IEEE Robotics and Automation Letters, 2023; IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)

[Code](https://github.com/yanglh14/TacGNN), [Paper](https://bionicdl.ancorasir.com/wp-content/uploads/2023/11/2023-J-RAL-TacGNN.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">RAL 2021</div><img src='images/Interactive-grasping.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learning-based optoelectronically innervated tactile finger for rigid-soft interactive grasping](https://arxiv.org/pdf/2101.12379)

**Linhan Yang**, Xudong Han, Weijie Guo, Fang Wan, Jia Pan, Chaoyang Song

IEEE Robotics and Automation Letters, 2021; IEEE International Conference on Robotics and Automation (ICRA)

[Code](https://github.com/yanglh14/InteractiveGrasping), [Paper](https://arxiv.org/pdf/2101.12379)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">RAL 2020</div><img src='images/Rigid-soft.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Rigid-soft interactive learning for robust grasping](https://arxiv.org/pdf/2003.01584)

**Linhan Yang**, Fang Wan, Haokun Wang, Xiaobo Liu, Yujia Liu, Jia Pan, Chaoyang Song

IEEE Robotics and Automation Letters, 2020; IEEE International Conference on Robotics and Automation (ICRA)

[Paper](https://arxiv.org/pdf/2003.01584)
</div>
</div>

-CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots;
Zeqing Zhang\*, **Linhan Yang**\*, Cong Sun, Weiwei Shang, Jia Pan;
[Paper](https://arxiv.org/pdf/2402.18420)

-DeepClaw: A robotic hardware benchmarking platform for learning object manipulation;
Fang Wan, Haokun Wang, Xiaobo Liu, Linhan Yang, Chaoyang Song;
2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM);
[Paper](https://arxiv.org/pdf/2005.02588)

\* Equal contribution